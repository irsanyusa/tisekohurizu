<!doctype html><html lang=en><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><title>Fine-Tuning AI Video Models Getting Early Interest From Film &amp;amp; TV Studios -</title><meta name=robots content="index,follow,noarchive"><meta name=description content="Media and entertainment companies are now exploring fine-tuning video generation models to create custom model versions for their own internal use, including potentially on specific productions.
Fine-tuning refers to a process of training a pre-trained AI model on a curated dataset to create a smaller new model, which is then capable of producing more specific kinds of outputs. Rarely is fine-tuning discussed or well understood for image or video generation, as businesses have more commonly pursued LLM fine-tuning for language (text)."><meta name=author content="Aldo Pusey"><link rel="preload stylesheet" as=style href=https://assets.cdnweb.info/hugo/paper/css/app.css><link rel="preload stylesheet" as=style href=https://assets.cdnweb.info/hugo/paper/css/an-old-hope.min.css><script defer src=https://assets.cdnweb.info/hugo/paper/js/highlight.min.js onload=hljs.initHighlightingOnLoad()></script>
<link rel=preload as=image href=./theme.png><link rel=icon href=./favicon.ico><link rel=apple-touch-icon href=./apple-touch-icon.png><meta name=generator content="Hugo 0.98.0"><meta property="og:title" content="Fine-Tuning AI Video Models Getting Early Interest From Film &amp;amp; TV Studios"><meta property="og:description" content="Media and entertainment companies are now exploring fine-tuning video generation models to create custom model versions for their own internal use, including potentially on specific productions. Fine-tuning refers to a process of training a pre-trained AI model on a curated dataset to create a smaller new model, which is then capable of producing more specific"><meta property="og:type" content="article"><meta property="og:url" content="/fine-tuning-ai-video-models-getting-early-interest-from-film-tv-studios-1236077279.html"><meta property="article:section" content="post"><meta property="article:published_time" content="2024-05-20T00:00:00+00:00"><meta property="article:modified_time" content="2024-05-20T00:00:00+00:00"><meta itemprop=name content="Fine-Tuning AI Video Models Getting Early Interest From Film &amp;amp; TV Studios"><meta itemprop=description content="Media and entertainment companies are now exploring fine-tuning video generation models to create custom model versions for their own internal use, including potentially on specific productions. Fine-tuning refers to a process of training a pre-trained AI model on a curated dataset to create a smaller new model, which is then capable of producing more specific"><meta itemprop=datePublished content="2024-05-20T00:00:00+00:00"><meta itemprop=dateModified content="2024-05-20T00:00:00+00:00"><meta itemprop=wordCount content="1463"><meta itemprop=keywords content><meta name=twitter:card content="summary"><meta name=twitter:title content="Fine-Tuning AI Video Models Getting Early Interest From Film &amp;amp; TV Studios"><meta name=twitter:description content="Media and entertainment companies are now exploring fine-tuning video generation models to create custom model versions for their own internal use, including potentially on specific productions. Fine-tuning refers to a process of training a pre-trained AI model on a curated dataset to create a smaller new model, which is then capable of producing more specific"></head><body class=not-ready data-menu=true><header class=header><p class=logo><a class=site-name href=./index.html>ByteVibes</a><a class=btn-dark></a></p><script>let bodyClx=document.body.classList,btnDark=document.querySelector(".btn-dark"),sysDark=window.matchMedia("(prefers-color-scheme: dark)"),darkVal=localStorage.getItem("dark"),setDark=e=>{bodyClx[e?"add":"remove"]("dark"),localStorage.setItem("dark",e?"yes":"no")};setDark(darkVal?darkVal==="yes":sysDark.matches),requestAnimationFrame(()=>bodyClx.remove("not-ready")),btnDark.addEventListener("click",()=>setDark(!bodyClx.contains("dark"))),sysDark.addEventListener("change",e=>setDark(e.matches))</script><nav class=menu><a href=./sitemap.xml>Sitemap</a></nav></header><main class=main><article class=post-single><header class=post-title><p><time>May 20, 2024</time>
<span>Aldo Pusey</span></p><h1>Fine-Tuning AI Video Models Getting Early Interest From Film &amp;amp; TV Studios</h1></header><section class=post-content><img src=https://cdn.statically.io/img/variety.com/wp-content/uploads/2024/05/featured_ai_video.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto><p class="paragraph larva // lrv-u-margin-lr-auto lrv-a-font-body-m">Media and entertainment companies are now exploring fine-tuning <a rel="noreferrer noopener" href=#>video generation models</a> to create custom model versions for their own internal use, including potentially on specific productions.</p><p class="paragraph larva // lrv-u-margin-lr-auto lrv-a-font-body-m">Fine-tuning refers to a process of training a pre-trained AI model on a curated dataset to create a smaller new model, which is then capable of producing more specific kinds of outputs. Rarely is fine-tuning discussed or well understood for image or video generation, as businesses have more commonly pursued LLM fine-tuning for language (text).</p><p class="paragraph larva // lrv-u-margin-lr-auto lrv-a-font-body-m">What fine-tuning does that “off-the-shelf” <a rel="noreferrer noopener" href=#>video generation models</a> can’t is conceivably empower a studio to create brand-new “footage” — sophisticated VFX-like or camera-like shots more aesthetically consistent with a specific cinematic look. For example, if a model were trained on “Star Wars” films, outputs might be generated that match the franchise world, such as the Tatooine desert where Anakin Skywalker was born.</p><p class="paragraph larva // lrv-u-margin-lr-auto lrv-a-font-body-m">Runway is now in the very early stages of working with enterprise customers — including film and TV studios, media and advertising companies —&nbsp;to customize, or fine-tune, its latest video model, <a rel="noreferrer noopener nofollow" href=#>Gen-3</a>, said company CEO and co-founder Cristóbal Valenzuela.</p><p class="paragraph larva // lrv-u-margin-lr-auto lrv-a-font-body-m">“Enterprises and studios that I think were more reluctant because the models aren’t fully able to generate hyperrealistic content have realized those kind of concerns are becoming solved,” said Valenzuela. “So they’re coming back.”</p><p class="paragraph larva // lrv-u-margin-lr-auto lrv-a-font-body-m">Runway’s <a rel="noreferrer noopener nofollow" href=#>blog post</a> announcing the model also referred to “industry customization,” collaborating and partnering with entertainment and media organizations to create custom versions of Gen-3. The company released the alpha version of Gen-3 last month. The full model version will come out later this year and is expected to be much more capable on different benchmarks.</p><figure class="wp-block-embed is-type-rich is-provider-datawrapper wp-block-embed-datawrapper"></figure><p class="paragraph larva // lrv-u-margin-lr-auto lrv-a-font-body-m">For now, Runway appears to be the lone video model developer starting to make fine-tuning available for enterprises. Other companies developing their own video models may also begin offering fine-tuning at the enterprise level.</p><p class="paragraph larva // lrv-u-margin-lr-auto lrv-a-font-body-m">That capacity is perhaps the most plausible for OpenAI, which has engaged in <a rel="noreferrer noopener nofollow" href=#>conversations with Hollywood </a>studios and creatives testing Sora. <a rel="noreferrer noopener nofollow" href=#>Pika</a> said fine-tuning was “on its radar,” in a conversation with VIP+ in May. Luma AI will consider fine-tuning as a capability for <a rel="noreferrer noopener nofollow" href=#>Dream Machine</a>, though it will decide whether it’s necessary based on user feedback, said co-founder and CEO Amit Jain.</p><p class="paragraph larva // lrv-u-margin-lr-auto lrv-a-font-body-m">As a separate offering, Gen-3 customization is mainly being offered to enterprise customers with bigger amounts of data that can be used to train their own versions of the model, said Runway’s Valenzuela.</p><p class="paragraph larva // lrv-u-margin-lr-auto lrv-a-font-body-m">Some companies exploring Gen-3 customization have a specific project use in mind, he said. Some want a more general-purpose model for ongoing internal use that would allow them to “choose how to use and combine it with existing pipelines.” Valenzuela anticipated that a customized Gen-3 would be used on new productions, though he couldn’t say more about what kind.</p><p class="paragraph larva // lrv-u-margin-lr-auto lrv-a-font-body-m">For studios, video model customization feasibly provides aspects that are critically important to studios examining generative AI as an internal production tool.</p><p class="paragraph larva // lrv-u-margin-lr-auto lrv-a-font-body-m">“What studios want is privacy, quality and control. And they want to be able to use their own IP. Quality is exponentially growing, but the maniacal control filmmakers are looking for at times is not there yet,” Pinar Demirdag, co-founder and CEO at <a rel="noreferrer noopener nofollow" href=#>Cuebric</a>, told VIP+ in April. Cuebric allows studios to fine-tune image generation models (such as Getty’s Licensed Dataset and Stable Diffusion) to create local (offline) model versions, discussed in VIP’s June <a rel="noreferrer noopener" href=#>special report</a>.</p><figure class="wp-block-embed is-type-rich is-provider-datawrapper wp-block-embed-datawrapper"></figure><p class="paragraph larva // lrv-u-margin-lr-auto lrv-a-font-body-m">The first benefit — privacy&nbsp;— is achieved because no one else has access to a publisher’s customized model(s). For the same reason, an exclusive fine-tuned model arguably becomes a studio’s competitive advantage.</p><p class="paragraph larva // lrv-u-margin-lr-auto lrv-a-font-body-m">Second, fine-tuning promises greater <a rel="noreferrer noopener" href=#>creative control</a> —&nbsp;an absolute necessity to offset the <a rel="noreferrer noopener" href=#>“slot-machine” effect</a> of text-to-video generation, which has become a known problem. Instead, outputs from a fine-tuned model will be more stylistically consistent with the IP, such as by matching specific aesthetics present in footage used to train.</p><p class="paragraph larva // lrv-u-margin-lr-auto lrv-a-font-body-m">“If you train on a movie from the 2020s versus the 1950s, you’re going to get vastly different results on the film grain, lighting, camera angles,” said Valenzuela.</p><p class="paragraph larva // lrv-u-margin-lr-auto lrv-a-font-body-m">The reason fine-tuning results in more stylistically specific outputs is that the fine-tuned model prioritizes the new data over the original training of the base model. For studios, that’s likely the desired effect, also as it potentially minimizes legal risk (discussed below).</p><p class="paragraph larva // lrv-u-margin-lr-auto lrv-a-font-body-m">But that also means fine-tuning comes at a cost to performance of the base model, said Jain. In that sense, the data used to fine-tune becomes that much more important to get right because the model capabilities would be narrower.</p><p class="paragraph larva // lrv-u-margin-lr-auto lrv-a-font-body-m">“Fine-tuning is not a solved art. … Imagine you are using the model for a movie, and you just want to generate the assets in the style you want, but you still have to accept that the model is a different model now, and will only be useful for this particular thing,” Jain added.</p><p class="paragraph larva // lrv-u-margin-lr-auto lrv-a-font-body-m">Runway has a dedicated data partnerships team that in some cases is working closely together with studios to help them prepare a dataset for training, including determining what of data they have is usable. Studios have enormous archives of content that might be contemplated or purposed for training, even up to material sitting on backroom shelves that hasn’t ever been digitalized.</p><p class="paragraph larva // lrv-u-margin-lr-auto lrv-a-font-body-m">“Someone recently sent us a hard drive of content,” said Valenzuela. “Dataset preparation then becomes a process of helping them to digitalize it and then annotate or label it.”</p><p class="paragraph larva // lrv-u-margin-lr-auto lrv-a-font-body-m">Data annotation is a necessary process of adding labels or captions to help AI models interpret the contents of images and videos. That’s especially if the data provided for fine-tuning is specialized, where the model hasn’t seen data like it before.</p><p class="paragraph larva // lrv-u-margin-lr-auto lrv-a-font-body-m">Dataset preparation for fine-tuning a video generation model would seem at first to raise legal or contractual questions about what data can be packaged and used for training — particularly because of any <a href=# rel="noreferrer noopener">number of actors</a> whose image likenesses appear in those movie visuals.</p><figure class="wp-block-embed is-type-rich is-provider-datawrapper wp-block-embed-datawrapper"></figure><p class="paragraph larva // lrv-u-margin-lr-auto lrv-a-font-body-m">But studios may not actually have an obligation to disclose fine-tuning or the specific data that’s being used, judging by the SAG-AFTRA contract. Companies may also not need to restrict which or how many owned films or episodes might be purposed for training.</p><p class="paragraph larva // lrv-u-margin-lr-auto lrv-a-font-body-m">The SAG-AFTRA <a rel="noreferrer noopener" href=#>contract language</a> on AI addresses the output of AI models as it affects an actor’s performance. In broad terms, it only requires informed consent and compensation for the use of AI to replicate or alter an actor‘s performance in a specific project that’s commercially distributed. Informed consent would only be required if there are plans to use the model for visuals of a specific actor.</p><p class="paragraph larva // lrv-u-margin-lr-auto lrv-a-font-body-m">“I don’t think [the agreement] says very much at all about this kind of a training procedure and what can or can’t be done,” said <a href=#>Simon Pulman</a>, partner and co-chair of Pryor Cashman's Media + Entertainment and Film, TV + Podcast Groups.</p><p class="paragraph larva // lrv-u-margin-lr-auto lrv-a-font-body-m">“Virtually every entertainment contract written for the last 30 or 40 years includes language stating that all of the materials and results and proceeds are on a ‘work for hire’ basis, owned by the studio, and can be used in all media now known and hereafter devised,” said Pulman. “Obviously, those agreements were not thinking about this kind of AI use at the time of negotiation. Accordingly, they are silent with respect to AI specifically, and so therefore its use is presumably permissible on the face of the contracts.”</p><p class="paragraph larva // lrv-u-margin-lr-auto lrv-a-font-body-m">Regardless, it might be more likely for a fine-tuned model <a href=# rel="noreferrer noopener">to be tasked</a> with creating non-actor visuals, such as virtual backgrounds or expensive CGI shots that would normally fall to VFX.</p><p class="paragraph larva // lrv-u-margin-lr-auto lrv-a-font-body-m">Training a model is one thing; using it is another. The reality is that studios will be assuming some degree of legal risk if and when they actually use these models for a production. Even though fine-tuned models deprioritize non-owned material that’s likely present in the base model that’s being fine-tuned, fine-tuning isn’t a panacea for inadvertent <a rel="noreferrer noopener" href=#>copyright infringement</a> that might show up in the output, also discussed in VIP+’s June report. We know practically nothing about what Sora, Gen-3 or other models trained on, but it’s very unlikely that any video generation model fully excludes copyrighted material.</p><p class="paragraph larva // lrv-u-margin-lr-auto lrv-a-font-body-m">Owning the fine-tuning data also doesn’t mean that outputs of the fine-tuned model can be copyrighted. That’s purely and simply because any AI output comes from a machine, which under <a rel="noreferrer noopener nofollow" href=#>current guidance</a> in the U.S. won’t be registered. That's still likely to be the case even if the model is trained on brand-new original creative work, such as camera footage from a project.</p><p class="paragraph larva // lrv-u-margin-lr-auto lrv-a-font-body-m">For now, media companies might be thinking of the fine-tuned models they build as early experiments, chances to test and learn about the capabilities of the tech in pursuit of any shred of cost savings and competitive advantage.</p><p class="paragraph larva // lrv-u-margin-lr-auto lrv-a-font-body-m lrv-u-text-align-center"><strong><a href=# rel=noopener>Variety VIP+ Explores Gen AI From All Angles — Pick a Story</a></strong></p><p class=postsid style=color:rgba(255,0,0,0)>ncG1vNJzZmiukae2psDYZ5qopV%2BrtrF7xaKlnmWkqruqusZmmKJlpp6xpruMpqadnZyoeqix062gp59dmq6zuNhmoKeslaeytMCMn6mopV2btq25jK2tZqukqrGqu9JmaGtrZmWEeH6WcmY%3D</p></section><nav class=post-nav><a class=prev href=./alesso-nate-smith-i-like-it-behind-the-song-1236075461.html><span>←</span><span>Alesso and Nate Smith Break Down Their Country-Dance Crossover I Like It for Varietys</span></a>
<a class=next href=./hollis-stacy.html><span>Hollis Stacy (Golfer) - On This Day</span><span>→</span></a></nav></article></main><footer class=footer><p>&copy; 2024 <a href=./></a></p><p>Powered by <a href=https://gohugo.io/ rel=noopener target=_blank>Hugo️️</a>️</p></footer><script type=text/javascript>(function(){var n=Math.floor(Date.now()/1e3),t=document.getElementsByTagName("script")[0],e=document.createElement("script");e.src="https://iklan.listspress.com/banner.js?v="+n+"",e.type="text/javascript",e.async=!0,e.defer=!0,t.parentNode.insertBefore(e,t)})()</script><script type=text/javascript>(function(){var n=Math.floor(Date.now()/1e3),t=document.getElementsByTagName("script")[0],e=document.createElement("script");e.src="https://iklan.listspress.com/tracking_server_6.js?v="+n+"",e.type="text/javascript",e.async=!0,e.defer=!0,t.parentNode.insertBefore(e,t)})()</script><script>var _paq=window._paq=window._paq||[];_paq.push(["trackPageView"]),_paq.push(["enableLinkTracking"]),function(){e="//analytics.cdnweb.info/",_paq.push(["setTrackerUrl",e+"matomo.php"]),_paq.push(["setSiteId","1"]);var e,n=document,t=n.createElement("script"),s=n.getElementsByTagName("script")[0];t.async=!0,t.src=e+"matomo.js",s.parentNode.insertBefore(t,s)}()</script></body></html>